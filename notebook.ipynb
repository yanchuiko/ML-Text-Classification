{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOD006562 - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Core Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (This is News_Categories.csv, I renamed it to dataset.csv for simplicity)\n",
    "df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 5 rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last 5 rows of the dataset\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows and Columns of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of the dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for Null Values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for Duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Null Values\n",
    "df = df.dropna()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicates\n",
    "df = df.drop_duplicates()\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns: authors, link, date\n",
    "df = df.drop(['authors', 'link', 'date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge headline and short_description into one column called 'text'\n",
    "df['text'] = df['headline'] + ' ' + df['short_description']\n",
    "\n",
    "# Drop headline and short_description columns because they are no longer needed\n",
    "df = df.drop(['headline', 'short_description'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the unique categories\n",
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of unique categories\n",
    "df['category'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of culture and arts categories\n",
    "culture_arts_categories = ['ARTS', 'CULTURE & ARTS', 'ARTS & CULTURE']\n",
    "\n",
    "# Array of news categories\n",
    "news_categories = ['WEIRD NEWS', 'WORLD NEWS', 'GOOD NEWS']\n",
    "\n",
    "# Array of voices categories\n",
    "voices_categories = ['LATINO VOICES', 'BLACK VOICES', 'QUEER VOICES']\n",
    "\n",
    "# Bundle the culture and arts categories into one category called 'CULTURE & ARTS'\n",
    "df['category'] = df['category'].replace(culture_arts_categories, 'CULTURE & ARTS')\n",
    "\n",
    "# Bundle the news categories into one category called 'NEWS'\n",
    "df['category'] = df['category'].replace(news_categories, 'NEWS')\n",
    "\n",
    "# Bundle the voices categories into one category called 'VOICES'\n",
    "df['category'] = df['category'].replace(voices_categories, 'VOICES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge categories with the same or similar meaning\n",
    "df['category'] = df['category'].replace('STYLE & BEAUTY', 'STYLE')\n",
    "df['category'] = df['category'].replace('PARENTING', 'PARENTS')\n",
    "df['category'] = df['category'].replace('COLLEGE', 'EDUCATION')\n",
    "df['category'] = df['category'].replace('TASTE', 'FOOD & DRINK')\n",
    "df['category'] = df['category'].replace('DIVORCE', 'WEDDINGS')\n",
    "df['category'] = df['category'].replace('MONEY', 'BUSINESS')\n",
    "df['category'] = df['category'].replace('HEALTHY LIVING', 'WELLNESS')\n",
    "df['category'] = df['category'].replace('THE WORLDPOST', 'WORLDPOST')\n",
    "df['category'] = df['category'].replace('WORLDPOST', 'NEWS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop trash categories\n",
    "trash_categories = ['GREEN', 'FIFTY']\n",
    "df = df[~df['category'].isin(trash_categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the unique categories\n",
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of unique categories\n",
    "df['category'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the count of each category before Downsampling\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(df['category'])\n",
    "plt.title('Count of Each Category before Downsampling')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate the dataset into different categories\n",
    "politics = df[df['category'] == 'POLITICS']\n",
    "wellness = df[df['category'] == 'WELLNESS']\n",
    "entertainment = df[df['category'] == 'ENTERTAINMENT']\n",
    "other_categories = df[(df['category'] != 'POLITICS') & \n",
    "                      (df['category'] != 'WELLNESS') & \n",
    "                      (df['category'] != 'ENTERTAINMENT')]\n",
    "\n",
    "# Downsample the categories with more than 10000 samples\n",
    "politics = resample(politics, replace=False, n_samples=10000, random_state=42)\n",
    "wellness = resample(wellness, replace=False, n_samples=10000, random_state=42)\n",
    "entertainment = resample(entertainment, replace=False, n_samples=10000, random_state=42)\n",
    "\n",
    "# Combine the downsampled categories back into a single dataframe\n",
    "df_downsampled = pd.concat([politics, wellness, entertainment])\n",
    "\n",
    "# Merge the downsampled dataframe with the dataframe of other categories\n",
    "df = pd.concat([other_categories, df_downsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the count of each category after downsampling\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(df['category'])\n",
    "plt.title('Count of Each Category After Downsampling')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of samples in each category\n",
    "category_counts = df['category'].value_counts()\n",
    "\n",
    "# Get a list of categories that have less than 6000 samples\n",
    "categories_to_drop = category_counts[category_counts < 6000].index\n",
    "\n",
    "# Drop these categories from the dataframe\n",
    "df = df[~df['category'].isin(categories_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the count of each category after dropping categories with less than 6000 samples\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(df['category'])\n",
    "plt.title('Count of Each Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "from langdetect import detect\n",
    "\n",
    "# Function to detect the language\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "    \n",
    "# Apply the function to the 'text' column\n",
    "df['language'] = df['text'].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows where the language is not English\n",
    "df = df[df['language'] == 'en']\n",
    "\n",
    "# Drop the 'language' column\n",
    "df = df.drop('language', axis=1)\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Display the updated dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Function that Normalizes my 'text' column\n",
    "def clean_text(text):\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace all diacritical marks with their corresponding characters\n",
    "    text = unidecode(text)\n",
    "    \n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@[a-zA-Z0-9_]+', '', text)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    # Remove URLs    \n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text)\n",
    "    \n",
    "    # Remove new line characters\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    \n",
    "    # Remove punctuation and underscores\n",
    "    text = re.sub(r'[^\\w\\s]|_', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Return the cleaned text\n",
    "    return text\n",
    "\n",
    "# Here I apply the clean_text function to the 'text' column\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Display the updated dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the remove_stopwords function to the 'text' column\n",
    "df['text'] = df['text'].apply(remove_stopwords)\n",
    "\n",
    "# Display the updated dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to lemmatize the text\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the lemmatize_text function to the 'text' column\n",
    "df['text'] = df['text'].apply(lemmatize_text)\n",
    "\n",
    "# Display the updated dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
